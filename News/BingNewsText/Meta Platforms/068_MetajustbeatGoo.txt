{
    "authors": [
        "Michael Nu\u00f1ez"
    ],
    "date_download": "None",
    "date_modify": "None",
    "date_publish": "2024-10-24 18:57:42",
    "description": "Meta has launched compressed AI models that run directly on smartphones, making artificial intelligence faster and more private while using less memory than cloud-based alternatives.",
    "filename": ".json",
    "image_url": "https://venturebeat.com/wp-content/uploads/2024/10/nuneybits_Vector_art_of_a_quantum_atom_coming_out_of_a_smartpho_b79cd110-dbb1-4e55-a187-eba70c8451d1.webp?w=986?w=1200&strip=all",
    "language": "en",
    "localpath": null,
    "maintext": "Meta Platforms has created smaller versions of its Llama artificial intelligence models that can run on smartphones and tablets, opening new possibilities for AI beyond data centers.\nThe company announced compressed versions of its Llama 3.2 1B and 3B models today that run up to four times faster while using less than half the memory of earlier versions. These smaller models perform nearly as well as their larger counterparts, according to Meta\u2019s testing.\nHow Meta made large language models work on phones\nThe advancement uses a compression technique called quantization, which simplifies the mathematical calculations that power AI models. Meta combined two methods: Quantization-Aware Training with LoRA adaptors (QLoRA) to maintain accuracy, and SpinQuant to improve portability.\nThis technical achievement solves a key problem: running advanced AI without massive computing power. Until now, sophisticated AI models required data centers and specialized hardware.\nTests on OnePlus 12 Android phones showed the compressed models were 56% smaller and used 41% less memory while processing text more than twice as fast. The models can handle texts up to 8,000 characters, enough for most mobile apps.\nTech giants race to define AI\u2019s mobile future\nMeta\u2019s release intensifies a strategic battle among tech giants to control how AI runs on mobile devices. While Google and Apple take careful, controlled approaches to mobile AI \u2014 keeping it tightly integrated with their operating systems \u2014 Meta\u2019s strategy is markedly different.\nBy open-sourcing these compressed models and partnering with chip makers Qualcomm and MediaTek, Meta bypasses traditional platform gatekeepers. Developers can build AI applications without waiting for Google\u2019s Android updates or Apple\u2019s iOS features. This move echoes the early days of mobile apps, when open platforms dramatically accelerated innovation.\nThe partnerships with Qualcomm and MediaTek are particularly significant. These companies power most of the world\u2019s Android phones, including devices in emerging markets where Meta sees growth potential. By optimizing its models for these widely-used processors, Meta ensures its AI can run efficiently on phones across different price points \u2014 not just premium devices.\nThe decision to distribute through both Meta\u2019s Llama website and Hugging Face, the increasingly influential AI model hub, shows Meta\u2019s commitment to reaching developers where they already work. This dual distribution strategy could help Meta\u2019s compressed models become the de facto standard for mobile AI development, much as TensorFlow and PyTorch became standards for machine learning.\nThe future of AI in your pocket\nMeta\u2019s announcement today points to a larger shift in artificial intelligence: the move from centralized to personal computing. While cloud-based AI will continue to handle complex tasks, these new models suggest a future where phones can process sensitive information privately and quickly.\nThe timing is significant. Tech companies face mounting pressure over data collection and AI transparency. Meta\u2019s approach \u2014 making these tools open and running them directly on phones \u2014 addresses both concerns. Your phone, not a distant server, could soon handle tasks like document summarization, text analysis, and creative writing.\nThis mirrors other pivotal shifts in computing. Just as processing power moved from mainframes to personal computers, and computing moved from desktops to smartphones, AI appears ready for its own transition to personal devices. Meta\u2019s bet is that developers will embrace this change, creating applications that blend the convenience of mobile apps with the intelligence of AI.\nSuccess isn\u2019t guaranteed. These models still need powerful phones to run well. Developers must weigh the benefits of privacy against the raw power of cloud computing. And Meta\u2019s competitors, particularly Apple and Google, have their own visions for AI\u2019s future on phones.\nBut one thing is clear: AI is breaking free from the data center, one phone at a time.",
    "source_domain": null,
    "text": null,
    "title": "Meta just beat Google and Apple in the race to put powerful AI on phones",
    "title_page": null,
    "title_rss": null,
    "url": null
}